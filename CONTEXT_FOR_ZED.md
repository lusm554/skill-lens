# SkillLens: Контекст разработки

## 1. О проекте
**SkillLens** — пет-проект для анализа рынка труда (Data Science Portfolio).
**Цель:** Сбор вакансий с HH.ru, анализ навыков, построение графа знаний.
**Текущий этап:** Реализация **ETL пайплайна (Bronze Layer)**.

## 2. Архитектура и Стек
*   **Архитектура:** Data Lakehouse (Medallion: Bronze -> Silver -> Gold).
*   **Оркестрация:** Apache Airflow 3.1.3 (в Docker).
*   **Хранилище:** MinIO (S3-compatible).
*   **Формат данных Bronze:** Сжатые JSONL (`.jsonl.gz`), партиционированные по дате.
*   **Стек скриптов:** Python, `asyncio`, `aiohttp`, `airflow.sdk` (TaskFlow API).

## 3. Инфраструктура (Docker Compose)
Проект запускается через `docker-compose.yaml` в корне.
*   **Сервисы:** Airflow (Web, Scheduler, Triggerer, Worker), Postgres, Redis, MinIO.
*   **Секреты:** Переменные `CLIENT_ID` и `CLIENT_SECRET` прокидываются из файла `airflow/.hhru_env` в переменные окружения контейнеров.
*   **Auth Token:** Хранится и обновляется в **Airflow Variables** (`HH_AUTH_TOKEN_INFO`), чтобы переживать рестарты тасков.

## 4. Текущая задача: Bronze Loader DAG
Мы разработали DAG `bronze_hh_loader_v3` для выгрузки вакансий за прошедшие сутки.

### Проблема API HH.ru
1.  API отдает максимум **2000** вакансий (глубина 2000).
2.  Если вакансий больше, нужно дробить запрос.
3.  Дробление по времени с точностью до секунд (`12:00:01`) вызывает **дубликаты**, так как индекс HH обновляется не мгновенно.

### Решение: Алгоритм "Hybrid Walker"
Мы отказались от секундной нарезки в пользу гибридной стратегии:
1.  **Time Slicing:** Уменьшаем окно времени, но **не меньше 1 минуты**.
2.  **Param Drilling:** Если за 1 минуту всё еще > 2000 вакансий, дробим запрос по бизнес-параметрам:
    *   Сначала по `employment_form` (Тип занятости).
    *   Если всё еще много — по `work_format` (Формат работы).
3.  **Hard Cap:** Если даже при максимальном дроблении > 2000, забираем первые 2000 и логируем warning (принимаем потерю хвоста).
4.  **Batching:** Данные копятся в памяти по 10,000 шт, сжимаются в `.jsonl.gz` и улетают в S3 частями (`part_0001.jsonl.gz`).
